# Repository UCI
This repository contains the project work done for my Coursera class __Getting and Cleaning Data__.

The purpose of this project is to demonstrate my ability to collect, work with, and clean a data set. The goal is to prepare a dataset which meets the principles of tidy data^3^ and that can be used for later analysis.  To this end, the file __run_analysis.R__ contains the R code necessary for the task.

The motivation for the project comes from research being done in the area of wearable computing^1^.  The data used in the project is data collected from the accelerometers of the Samsung Galaxy S smartphone. A full description is available at the site where the data was obtained^2^.

Guidelines for approaching the project work can be found in the Coursera Course Project Forum^4^, ^5^.

In addition to this README, there is a file in the repository named CodeBook.md which describes the variables, the data, and any transformations or work performed to clean up the data.

## Environment
* __R version__: R version 3.2.0 (2015-04-16)
* __R Studio version__: RStudio Desktop 0.99.467
* __Operating environment(s)__: Mac OS/X Yosemite, Ubuntu 14.04, RHEL 6.6
* __Dependencies__: This script will require the following library to be installed and available - dplyr

**NOTE**:  This script was developed and executed on Unix-like platforms and makes no provisions for running on a non-Unix platform.  A future version may take into consideration the target execution platform and adjust paths accordingly.

## Overview of run_analysis.R
0. Obtain and extract the data
1. Merge the training and the test sets to create one data set
2. Extract only the columns containing values for the mean and standard deviation of each measurement
3. Use descriptive activity names to name the activities in the data set
4. Appropriately label the data set columns with descriptive variable names
5. From the data set in step 4, create a second, independent tidy data set with the average of each variable for each activity and each subject

## Detailed explanation
### Step 0 - Obtain and extract the data  
The dataset under analysis (UCI HAR Dataset) is obtained from the UCI Machine Learning Repository^2^.  The script contains code to:  
1. set the working directory to the directory from which the script is run
2. check for the existence of the compressed data file
3. if the compressed data file does not exist:
+ the file will be downloaded as *data.zip* from the link pointing to the UCI Machine Learning Repository^2^
+ the compressed file will be extracted to the *data* directory, relative to the current working directory
+ the files for analysis will reside in *data/UCI HAR Dataset*, *data/UCI HAR Dataset/train*, and *data/UCI HAR Dataset/test*  

**NOTE**:  The code to do the actual download and extraction has been commented out due to the prohibitive time of the download.  Prior to executing the script, it is expected that the user of the script has downloaded the file directly from the page link (__*much*__ faster!) and extracted the contents to the following directories: *./data*, *./data/train*, and *./data/test*.  Step 1 of the script will verify the existence of this directory and exit the script should it not exist.

### Step 1 - Merge the training and the test sets to create one data set

### Step 2 - Extract only the columns containing values for the mean and standard deviation of each measurement

### Step 3 - Use descriptive activity names to name the activities in the data set

### Step 4 - Appropriately label the data set columns with descriptive variable names

### Step 5 - From the data set in step 4, create a second, independent tidy data set with the average of each variable for each activity and each subject

Script checks only for the existence of the directory "./data", which is relative to the location of the script.  If this directory does not exist, the script exits with an appropriate message.

Indicate that the Inertial files were not used; the files you used are a sample of that data



Add to the submission box:
"tidy data as per the ReadMe that can be read into R with read.table(header=TRUE)"



Make sure:
1. headings clearly identify which columns are which
2. variables are in different columns
3. no duplicates (check!)






Tidy data is not made to be looked neatly at in programs like notepad (which is often the default for text files on windows), but if you saved the file with write.table according to the instructions, the command for reading it in and looking at it in R would be

    data <- read.table(file_path, header = TRUE) #if they used some other way of saving the file than a default write.table, this step will be different
    View(data)
    
   
   Over in the FAQ thread I mentioned people could put the code for reading the tidy data into R into their readMe

But if you want to get fancier than that:

If you right click on a file name in the submission box and copy the web address, you can paste it into a script.

   address <- "https://s3.amazonaws.com/coursera-uploads/user-longmysteriouscode/asst-3/massivelongcode.csv"
address <- sub("^https", "http", address)
data <- read.table(url(address), header = TRUE) #if they used some other way of saving the file than a default write.table, this step will be different
View(data)





##### __References__:
###### 1. *Data Science, Wearable Computing and the Battle for the Throne as Worldâ€™s Top Sports Brand* (http://www.insideactivitytracking.com/data-science-activity-tracking-and-the-battle-for-the-worlds-top-sports-brand/)
###### 2. *Human Activity Recognition Using Smartphones Data Set* (http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) 
###### 3. *Hadley Wickham's paper on Tidy Data* (http://www.jstatsoft.org/v59/i10/paper)
###### 4. *David's personal course project FAQ* (https://class.coursera.org/getdata-030/forum/thread?thread_id=37)
###### 5. *Tidy Data and the Assignment* (https://class.coursera.org/getdata-030/forum/thread?thread_id=107)

---
title: "ReadMe"
author: "Kevin E. D'Elia"
date: "07/22/2015"
output: html_document
---